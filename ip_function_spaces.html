
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3. Linear inverse problems in function spaces &#8212; 10 Lectures on Inverse Problems and Imaging</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="4. A statistical perspective on inverse problems" href="statistical_perspective.html" />
    <link rel="prev" title="2. Discrete Inverse Problems and Regularisation" href="discrete_ip_regularization.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">10 Lectures on Inverse Problems and Imaging</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome to Inverse Problems and Imaging
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Lectures
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="what_is.html">
   1. What is an inverse problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="discrete_ip_regularization.html">
   2. Discrete Inverse Problems and Regularisation
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   3. Linear inverse problems in function spaces
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="statistical_perspective.html">
   4. A statistical perspective on inverse problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="variational_formulations.html">
   5. Variational formulations for inverse problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="numerical_optimisation.html">
   6. Numerical optimisation for inverse problems
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="image_processing.html">
   7. Image processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tomography.html">
   8. Computed Tomography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wavefield_imaging.html">
   9. Wavefield Imaging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="magnetic_resonance_imaging.html">
   10. Magnetic Resonance Imaging
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography.html">
   11. References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="_sources/ip_function_spaces.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/ip_function_spaces.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        
        <a class="edit-button" href="https://github.com/TristanvanLeeuwen/IP_and_Im_Lectures/edit/master/ip_function_spaces.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/TristanvanLeeuwen/IP_and_Im_Lectures/master?urlpath=tree/ip_function_spaces.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#well-posedness">
   3.1. Well-posedness
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bounded-operators-on-hilbert-spaces">
   3.2. Bounded operators on Hilbert spaces
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#compact-operators">
   3.3. Compact operators
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularisation">
   3.4. Regularisation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#truncation-and-tikhonov-regularisation">
     3.4.1. Truncation and Tikhonov regularisation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generalised-tikhonov-regularisation">
     3.4.2. Generalised Tikhonov regularisation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parameter-choice-strategies">
     3.4.3. Parameter-choice strategies
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   3.5. Exercises
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolution">
     3.5.1. Convolution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#differentiation">
     3.5.2. Differentiation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convergent-regularisation">
     3.5.3. Convergent regularisation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolution-through-the-heat-equation">
     3.5.4. Convolution through the heat equation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#assignments">
   3.6. Assignments
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discretisation">
     3.6.1. Discretisation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolution-on-a-finite-interval">
     3.6.2. Convolution on a finite interval
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Linear inverse problems in function spaces</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#well-posedness">
   3.1. Well-posedness
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bounded-operators-on-hilbert-spaces">
   3.2. Bounded operators on Hilbert spaces
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#compact-operators">
   3.3. Compact operators
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularisation">
   3.4. Regularisation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#truncation-and-tikhonov-regularisation">
     3.4.1. Truncation and Tikhonov regularisation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generalised-tikhonov-regularisation">
     3.4.2. Generalised Tikhonov regularisation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parameter-choice-strategies">
     3.4.3. Parameter-choice strategies
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   3.5. Exercises
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolution">
     3.5.1. Convolution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#differentiation">
     3.5.2. Differentiation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convergent-regularisation">
     3.5.3. Convergent regularisation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolution-through-the-heat-equation">
     3.5.4. Convolution through the heat equation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#assignments">
   3.6. Assignments
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discretisation">
     3.6.1. Discretisation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolution-on-a-finite-interval">
     3.6.2. Convolution on a finite interval
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="linear-inverse-problems-in-function-spaces">
<h1><span class="section-number">3. </span>Linear inverse problems in function spaces<a class="headerlink" href="#linear-inverse-problems-in-function-spaces" title="Permalink to this headline">¶</a></h1>
<p>Many of the notions discussed in the finite-dimensional setting can be extended to the infinite-dimensional setting. We will focus in this chapter on inverse problems where <span class="math notranslate nohighlight">\(K\)</span> is a <a class="reference external" href="https://en.wikipedia.org/wiki/Bounded_operator"><em>bounded linear operator</em></a> and <span class="math notranslate nohighlight">\(\mathcal{U}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{V}\)</span> are (infinite-dimensional) function spaces. The contents of this chapter were heavily inspired by the excellent <a class="reference external" href="https://mehrhardt.github.io/data/201803_lecture_notes_invprob.pdf">lecture notes from Matthias J. Ehrhardt and Lukas F. Lang</a>.</p>
<p>Let <span class="math notranslate nohighlight">\(K: \mathcal{U} \rightarrow F\)</span> denote the forward operator, with <span class="math notranslate nohighlight">\(\mathcal{U}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{V}\)</span> <a class="reference external" href="https://en.wikipedia.org/wiki/Banach_space">Banach spaces</a>. The operator is bounded iff there exists a constant <span class="math notranslate nohighlight">\(C \geq 0\)</span> such that</p>
<div class="math notranslate nohighlight">
\[\|Ku\|_{\mathcal{F}} \leq C \|u\|_{\mathcal{U}} \quad \forall u \in \mathcal{U}.\]</div>
<p>The smallest such constant is called the operator norm <span class="math notranslate nohighlight">\(\|K\|\)</span>. Note that boundedness also implies continuity, i.e. for any <span class="math notranslate nohighlight">\(u,v \in \mathcal{U}\)</span> we have</p>
<div class="math notranslate nohighlight">
\[\|Ku - Kv\|_{\mathcal{F}} \leq C \|u - v\|_{\mathcal{U}}.\]</div>
<div class="section" id="well-posedness">
<h2><span class="section-number">3.1. </span>Well-posedness<a class="headerlink" href="#well-posedness" title="Permalink to this headline">¶</a></h2>
<p>We may again wonder whether the equation <span class="math notranslate nohighlight">\(Ku = f\)</span> is well-posed. To formally analyse this we introduce the following notation:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}(K) = \mathcal{U}\)</span> denotes the <em>domain</em> of <span class="math notranslate nohighlight">\(K\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{R}(K) = \{Ku\,|\,u\in\mathcal{U}\}\)</span> denotes the <em>range</em> of <span class="math notranslate nohighlight">\(K\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{N}(K) = \{u\in\mathcal{U}\, | \, Ku = 0 \}\)</span> denotes the <em>null-space</em> or <em>kernel</em> of <span class="math notranslate nohighlight">\(K\)</span>.</p></li>
</ul>
<p>When <span class="math notranslate nohighlight">\(f \not\in \mathcal{R}(K)\)</span>, a solution obviously doesn’t exist. We can still look for a solution for which <span class="math notranslate nohighlight">\(Ku \approx f\)</span> by solving</p>
<div class="math notranslate nohighlight" id="equation-minres">
<span class="eqno">(3.1)<a class="headerlink" href="#equation-minres" title="Permalink to this equation">¶</a></span>\[\min_{u\in \mathcal{U}}\|Ku - f\|_{\mathcal{F}}.\]</div>
<p>A solution to <a class="reference internal" href="#equation-minres">(3.1)</a> is a vector <span class="math notranslate nohighlight">\(\widetilde{u} \in \mathcal{U}\)</span> for which</p>
<div class="math notranslate nohighlight">
\[\|K\widetilde{u} - f\|_{\mathcal{F}} \leq \|Ku - f\|_{\mathcal{F}}, \quad \forall u \in \mathcal{U}.\]</div>
<p>If such a vector exists we call it the <em>minimum-residual</em> solution. If the null-space of <span class="math notranslate nohighlight">\(K\)</span> is non-empty, we can construct infinitely many such solutions. We call the one with the smallest norm the <em>minimum-norm</em> solution. Note however that we have not yet proven that such a solution exists in general, nor do we have a constructive way of finding it.</p>
</div>
<div class="section" id="bounded-operators-on-hilbert-spaces">
<h2><span class="section-number">3.2. </span>Bounded operators on Hilbert spaces<a class="headerlink" href="#bounded-operators-on-hilbert-spaces" title="Permalink to this headline">¶</a></h2>
<p>To study well-posedness of <a class="reference internal" href="#equation-minres">(3.1)</a> and the corresponding minimum-norm problem we will let <span class="math notranslate nohighlight">\(\mathcal{U}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> be <a class="reference external" href="https://en.wikipedia.org/wiki/Hilbert_space">Hilbert spaces</a>. We will return to analysing variational problems more generally in a <a class="reference internal" href="variational_formulations.html"><span class="doc std std-doc">later chapter</span></a>.</p>
<hr class="docutils" />
<p>First, we introduce the <a class="reference external" href="https://en.wikipedia.org/wiki/Transpose_of_a_linear_map">adjoint</a> of <span class="math notranslate nohighlight">\(K\)</span>, denoted by <span class="math notranslate nohighlight">\(K^*\)</span> in the usual way as satisfying</p>
<div class="math notranslate nohighlight">
\[\langle Ku, f \rangle_{\mathcal{F}} = \langle K^*\!f, u \rangle_{\mathcal{U}}\quad \forall u\in\mathcal{U}, \forall f\in\mathcal{F}.\]</div>
<p>We further denote the <em>orthogonal complement</em> of a subspace <span class="math notranslate nohighlight">\(\mathcal{X} \subset \mathcal{U}\)</span> as</p>
<div class="math notranslate nohighlight">
\[\mathcal{X}^\perp = \{u\in\mathcal{U} \, | \, \langle u, v \rangle_{\mathcal{U}}=0 \, \forall \, v \in \mathcal{X}\}.\]</div>
<p>If <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> is a <a class="reference external" href="https://en.wikipedia.org/wiki/Closed_set">closed subspace</a> we have <span class="math notranslate nohighlight">\((\mathcal{X}^\perp)^\perp = \mathcal{X}\)</span> and we have an orthogonal decomposition of <span class="math notranslate nohighlight">\(\mathcal{U}\)</span> given by <span class="math notranslate nohighlight">\(\mathcal{U} = \mathcal{X} \oplus \mathcal{X}^\perp\)</span>, meaning that we can express <em>any</em> <span class="math notranslate nohighlight">\(u\in \mathcal{U}\)</span> as <span class="math notranslate nohighlight">\(u = x + x^\perp\)</span> with <span class="math notranslate nohighlight">\(x\in\mathcal{X}\)</span> and <span class="math notranslate nohighlight">\(x^\perp\in\mathcal{X}^\perp\)</span>.
The <a class="reference external" href="https://en.wikipedia.org/wiki/Projection_(linear_algebra)">orthogonal projection</a> onto <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> is denoted by <span class="math notranslate nohighlight">\(P_{\mathcal{X}}\)</span>. We briefly recall a few usefull relations</p>
<div class="important admonition">
<p class="admonition-title">Lemma : <em>Orthogonal projection</em></p>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{X} \subset \mathcal{U}\)</span> be a closed subspace. The orthogonal projection onto <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> satisfies the following conditions</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(P_{\mathcal{X}}\)</span> is self-adjoint,</p></li>
<li><p><span class="math notranslate nohighlight">\(\|P_{\mathcal{X}}\| = 1,\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(I - P_{\mathcal{X}} = P_{\mathcal{X}^\perp},\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\|u - P_{\mathcal{X}}u\|_{\mathcal{U}} \leq \|u - v\|_{\mathcal{U}} \, \forall \, v\in\mathcal{X}\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(v = P_{\mathcal{X}}u\)</span> iff <span class="math notranslate nohighlight">\(v\in\mathcal{X}\)</span> and <span class="math notranslate nohighlight">\(u-v\in\mathcal{X}^\perp\)</span>.</p></li>
</ol>
</div>
<p>When <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> is not closed we have <span class="math notranslate nohighlight">\((\mathcal{X}^\perp)^\perp = \overline{\mathcal{X}}\)</span> (the <a class="reference external" href="https://en.wikipedia.org/wiki/Closure_(topology)">closure</a> of <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>). Note that the orthogonal complement of a subspace is always closed.</p>
<p>We now have the following usefull relations</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{R}(K)^\perp = \mathcal{N}(K^*),\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{N}(K^*)^\perp = \overline{\mathcal{R}(K)},\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{R}(K^*)^\perp = \mathcal{N}(K),\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{N}(K)^\perp = \overline{\mathcal{R}(K^*)},\)</span></p></li>
</ul>
<p>from which we conclude that we can decompose <span class="math notranslate nohighlight">\(\mathcal{U}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> as</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{U} = \mathcal{N}(K) \oplus \overline{\mathcal{R}(K^*)},\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{F} = \mathcal{N}(K^*) \oplus \overline{\mathcal{R}(K)}.\)</span></p></li>
</ul>
<hr class="docutils" />
<p>We now have the following results</p>
<div class="important admonition">
<p class="admonition-title">Theorem: <em>Existence and uniqueness of the minimum-residual, minimum-norm solution</em></p>
<p>Given a bounded linear operator <span class="math notranslate nohighlight">\(K\)</span> and <span class="math notranslate nohighlight">\(f \in \mathcal{F}\)</span>, a solution <span class="math notranslate nohighlight">\(\widetilde{u}\)</span> to <a class="reference internal" href="#equation-minres">(3.1)</a></p>
<ul class="simple">
<li><p>only exists if <span class="math notranslate nohighlight">\(f \in \mathcal{R}(K) \oplus \mathcal{R}(K)^\perp\)</span></p></li>
<li><p>obeys the <em>normal equations</em> <span class="math notranslate nohighlight">\(K^*\! K\widetilde{u} = K^*\! f.\)</span></p></li>
</ul>
<p>The unique solution <span class="math notranslate nohighlight">\(\widetilde{u} \in \mathcal{N}(K)^{\perp}\)</span> to the normal equations is called the <em>minimum-norm</em> solution.</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Proof</p>
<p>First, we show that a minimum-residual solution necessarily obeys the normal equations:</p>
<ul class="simple">
<li><p>Given a solution <span class="math notranslate nohighlight">\(\widetilde{u}\)</span> to <a class="reference internal" href="#equation-minres">(3.1)</a> and an arbritary <span class="math notranslate nohighlight">\(v\in\mathcal{U}\)</span>, define <span class="math notranslate nohighlight">\(\phi(\alpha) = \|K(\widetilde{u} + \alpha v) - f\|_{\mathcal{F}}\)</span>. A necessary condition for <span class="math notranslate nohighlight">\(\widetilde{u}\)</span> to be a least-squares solution is <span class="math notranslate nohighlight">\(\phi'(0) = 0\)</span>, which gives <span class="math notranslate nohighlight">\(\langle K^*(K\widetilde{u}-f),v \rangle_{\mathcal{U}}\)</span>. As this should hold for arbritary <span class="math notranslate nohighlight">\(v\)</span>, we find the normal equations. Note that this also implies that <span class="math notranslate nohighlight">\(K\widetilde{u} - f \in \mathcal{R}(K)^\perp\)</span>.</p></li>
</ul>
<p>Next, we show that the normal equations only allow a solution when <span class="math notranslate nohighlight">\(f \in \mathcal{R}(K)^\perp \oplus \mathcal{R}(K)\)</span>.</p>
<ul class="simple">
<li><p>Given a solution <span class="math notranslate nohighlight">\(\widetilde{u}\)</span>, we find that <span class="math notranslate nohighlight">\(f = K\widetilde{u} + g\)</span> with <span class="math notranslate nohighlight">\(g\in\mathcal{R}(K)^\perp\)</span>. Hence, <span class="math notranslate nohighlight">\(f \in \mathcal{R}(K) \oplus \mathcal{R}(K)^\perp\)</span>. Conversely, given <span class="math notranslate nohighlight">\(f \in \mathcal{R}(K) \oplus \mathcal{R}(K)^\perp\)</span> , there exist <span class="math notranslate nohighlight">\(u \in \mathcal{U}\)</span> and <span class="math notranslate nohighlight">\(g \in \mathcal{R}(K)^\perp = \left(\overline{\mathcal{R}(K)}\right)^\perp\)</span> such that <span class="math notranslate nohighlight">\(f = Ku + g\)</span>. Thus, <span class="math notranslate nohighlight">\(P_{\overline{\mathcal{R}(K)}}f = Ku\)</span>. Such a <span class="math notranslate nohighlight">\(u \in \mathcal{U}\)</span> must necessarily be a solution to <a class="reference internal" href="#equation-minres">(3.1)</a> because we have <span class="math notranslate nohighlight">\(\|Ku - f\|_{\mathcal{F}} = \|P_{\overline{\mathcal{R}(K)}}f - f\|_{\mathcal{F}} \leq \inf_{g\in \overline{\mathcal{R}(K)}}\|g  - f\|_{\mathcal{F}} \leq \inf_{v\in\mathcal{U}}\|Kv - f\|_{\mathcal{F}}.\)</span> Here, we used the fact that the orthogonal projection on a subspace gives the element closest to the projected element and <span class="math notranslate nohighlight">\(\mathcal{R}(K) \subseteq \overline{\mathcal{R}(K)}\)</span> allows to conclude the last inequality.</p></li>
</ul>
<p>Finally, we show that the minimum-norm solution is unique. Denote the minimun-norm solution by <span class="math notranslate nohighlight">\(\widetilde{u}\)</span>. Now suppose we have another solution, <span class="math notranslate nohighlight">\(\widetilde{v}\)</span>, to <a class="reference internal" href="#equation-minres">(3.1)</a>. Since they both solve the normal equations we have <span class="math notranslate nohighlight">\(\widetilde{v} = \widetilde{u} + z\)</span> with <span class="math notranslate nohighlight">\(z \in \mathcal{N}(K)\)</span>. It follows that <span class="math notranslate nohighlight">\(\|\widetilde{v}\|_{\mathcal{U}}^2 = \|\widetilde{u}\|_{\mathcal{U}}^2 + 2\langle \widetilde{u}, z \rangle_{\mathcal{U}} + \|z\|_{\mathcal{U}}^2\)</span>. Since <span class="math notranslate nohighlight">\(\widetilde{u} \in \mathcal{N}(K)^\perp\)</span> we have  <span class="math notranslate nohighlight">\(\langle \widetilde{u}, z \rangle_{\mathcal{U}} = 0\)</span> and hence <span class="math notranslate nohighlight">\(\|\widetilde{v}\|_{\mathcal{U}} \geq \|\widetilde{u}\|_{\mathcal{U}}\)</span> with equality only obtained when <span class="math notranslate nohighlight">\(z = 0\)</span>.</p>
</div>
<hr class="docutils" />
<p>The Moore-Penrose pseudo-inverse of <span class="math notranslate nohighlight">\(K\)</span> gives us a way to construct the minimum-norm solution defined above. We can think of this pseudo-inverse as the inverse of a restricted forward operator.</p>
<div class="important admonition">
<p class="admonition-title">Definition: <em>Moore-Penrose inverse</em></p>
<p>Given an bounded linear operator <span class="math notranslate nohighlight">\(K:\mathcal{U}\rightarrow \mathcal{F}\)</span> we denote its restriction to <span class="math notranslate nohighlight">\(\mathcal{N}(K)^\perp\)</span> as <span class="math notranslate nohighlight">\(\widetilde{K}:\mathcal{N}(K)^\perp\rightarrow\mathcal{R}(K)\)</span>. The M-P inverse <span class="math notranslate nohighlight">\(K^\dagger: \mathcal{R}(K)\oplus\mathcal{R}(K)^\perp \rightarrow \mathcal{N}(K)^\perp\)</span> is defined as the unique linear extension of <span class="math notranslate nohighlight">\(\widetilde{K}^{-1}\)</span> with <span class="math notranslate nohighlight">\(\mathcal{N}(K^\dagger) = \mathcal{R}(K)^\perp\)</span>.</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title"><em>The range of <span class="math notranslate nohighlight">\(K^\dagger\)</span></em></p>
<p>Let <span class="math notranslate nohighlight">\(u\in\mathcal{R}(K^\dagger)\)</span>, then there exists a <span class="math notranslate nohighlight">\(f\in\mathcal{D}(K^\dagger)\)</span> such that <span class="math notranslate nohighlight">\(u = K^\dagger f\)</span>. By definition we can decompose <span class="math notranslate nohighlight">\(f = f_1 + f_2\)</span> with <span class="math notranslate nohighlight">\(f_1\in\mathcal{R}(K)\)</span>, <span class="math notranslate nohighlight">\(f_2 \in \mathcal{R}(K)^\perp\)</span>. Thus <span class="math notranslate nohighlight">\(K^\dagger f = K^\dagger f_1 = \widetilde{K}^{-1}f_1\)</span>. Hence, <span class="math notranslate nohighlight">\(u \in \mathcal{R}(\widetilde{K}^{-1}) = \mathcal{N}(K)^\perp\)</span>. This establishes that <span class="math notranslate nohighlight">\(\mathcal{R}(K^\dagger)\subseteq \mathcal{N}(K)^\perp\)</span>. Conversely, let <span class="math notranslate nohighlight">\(u\in\mathcal{N}(K)^\perp\)</span>. Then <span class="math notranslate nohighlight">\(u = \widetilde{K}^{-1}\widetilde{K} = K^\dagger K u\)</span>, whence <span class="math notranslate nohighlight">\(\mathcal{N}(K)^\perp \subseteq \mathcal{R}(K^\dagger)\)</span>. We conclude that <span class="math notranslate nohighlight">\(\mathcal{R}(K^\dagger) = \mathcal{N}(K)^\perp\)</span>.</p>
</div>
<p>The pseudo-inverse satisfies a few useful relations:</p>
<div class="important admonition">
<p class="admonition-title">Theorem: <em>Moore-Penrose equations</em></p>
<p>The M-P pseudo-inverse <span class="math notranslate nohighlight">\(K^{\dagger}: \mathcal{R}(K)^\perp \oplus \mathcal{R}(K) \rightarrow \mathcal{N}(K)^\perp\)</span> of <span class="math notranslate nohighlight">\(K\)</span> is unique and obeys the following useful relations:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(KK^\dagger = \left.P_{\overline{\mathcal{R}(K)}}\right|_{\mathcal{D}(K^\dagger)}.\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(K^\dagger K = I - P_{\mathcal{N}(K)},\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(K^\dagger K K^\dagger = K^\dagger,\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(KK^\dagger K = K,\)</span></p></li>
</ol>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Proof</p>
<ol class="simple">
<li><p>Decompose <span class="math notranslate nohighlight">\(f \in \mathcal{D}(K^\dagger)\)</span> as <span class="math notranslate nohighlight">\(f = f_1 + f_2\)</span> with <span class="math notranslate nohighlight">\(f_1\in\mathcal{R}(K)\)</span>, <span class="math notranslate nohighlight">\(f_2\in\mathcal{R}(K)^\perp\)</span> and use that <span class="math notranslate nohighlight">\(K = \widetilde{K}\)</span> on <span class="math notranslate nohighlight">\(\mathcal{N}(K)^\perp\)</span>. Then <span class="math notranslate nohighlight">\(KK^\dagger f = K\widetilde{K}^{-1}f_1 = f_1\)</span>. Hence, <span class="math notranslate nohighlight">\(KK^\dagger\)</span> acts as an orthogonal projection of <span class="math notranslate nohighlight">\(f \in \mathcal{D}(K^\dagger)\)</span> on <span class="math notranslate nohighlight">\(\overline{\mathcal{R}(K)}\)</span>.</p></li>
<li><p>Decompose <span class="math notranslate nohighlight">\(u \in \mathcal{U}\)</span> in two parts <span class="math notranslate nohighlight">\(u = u_1 + u_2\)</span> with <span class="math notranslate nohighlight">\(u_1 \in \mathcal{N}(K)\)</span>, <span class="math notranslate nohighlight">\(u_2\in\mathcal{N}(K)^\perp\)</span>
we have <span class="math notranslate nohighlight">\(K^\dagger K u = K^\dagger K u_1 = u_1\)</span>, so <span class="math notranslate nohighlight">\(KK^\dagger\)</span> acts like an orthogonal projection on <span class="math notranslate nohighlight">\(\mathcal{N}(K)^\perp\)</span> so <span class="math notranslate nohighlight">\(KK^\dagger = I - P_{\mathcal{N}(K}\)</span> (note that the orthogonal complement of a subspace is always closed).</p></li>
<li><p>Since <span class="math notranslate nohighlight">\(KK^\dagger\)</span>  projects onto <span class="math notranslate nohighlight">\(\overline{R}(K)\)</span>, it filters out any components in <span class="math notranslate nohighlight">\(f\)</span> in the null-space of <span class="math notranslate nohighlight">\(K^\dagger\)</span> which would disappear anyway.</p></li>
<li><p>Since <span class="math notranslate nohighlight">\(K^\dagger K\)</span> projects on <span class="math notranslate nohighlight">\(\mathcal{N}(K)^\perp\)</span> if filters out any components in the null-space of <span class="math notranslate nohighlight">\(K\)</span>, which would disappear anyway.</p></li>
</ol>
</div>
<p>With these, we can show that the minimum-norm solution to <a class="reference internal" href="#equation-minres">(3.1)</a> is obtained by applying the pseudo-inverse to the data.</p>
<div class="important admonition">
<p class="admonition-title">Theorem: <em>Minimum-norm solution</em></p>
<p>Let <span class="math notranslate nohighlight">\(K\)</span> be a bounded linear operator with pseudo-inverse <span class="math notranslate nohighlight">\(K^{\dagger}\)</span> and <span class="math notranslate nohighlight">\(f \in \mathcal{D}(K^\dagger)\)</span>, then the unique minimum-norm solution to <a class="reference internal" href="#equation-minres">(3.1)</a> is given by</p>
<div class="math notranslate nohighlight">
\[\widetilde{u} = K^\dagger f.\]</div>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Proof</p>
<p>We know that for <span class="math notranslate nohighlight">\(f \in \mathcal{D}(K^\dagger)\)</span> the minimum-norm solution <span class="math notranslate nohighlight">\(\widetilde{u} \in \mathcal{N}(K)^\perp\)</span> exists and unique. Using the fact that <span class="math notranslate nohighlight">\(K\widetilde{u} = P_{\overline{\mathcal{R}(K)}}f\)</span> (due to the \textit{mimimum-residual} solution property) and the M-R equations, we have <span class="math notranslate nohighlight">\(\widetilde{u} = (I - P_{\mathcal{N}}(K))\widetilde{u} = K^\dagger K\widetilde{u} = K^\dagger P_{\overline{\mathcal{R}(K)}}f = K^\dagger K K^\dagger f = K^\dagger f\)</span>.</p>
</div>
<p>When defining the the solution through the M-P pseudo-inverse, we have existence uniqueness of the minimum-norm to <a class="reference internal" href="#equation-minres">(3.1)</a>. However, we cannot expect stability in general. For this, we would need <span class="math notranslate nohighlight">\(K^{\dagger}\)</span> to be continuous. To see this, consider noisy data <span class="math notranslate nohighlight">\(f^{\delta} = f + e\)</span> with <span class="math notranslate nohighlight">\(\|e\|\leq \delta\)</span>. For stability of the solution we need to bound the difference between the corresponding solutions <span class="math notranslate nohighlight">\(\widetilde{u}\)</span>, <span class="math notranslate nohighlight">\(\widetilde{u}^\delta\)</span>:</p>
<div class="math notranslate nohighlight">
\[\|\widetilde{u} - \widetilde{u}^\delta\|_{\mathcal{U}} = \|K^{\dagger}e\|_{\mathcal{F}},\]</div>
<p>which we can only do when <span class="math notranslate nohighlight">\(K^\dagger\)</span> is continuous (or, equivalently, bounded).</p>
<div class="important admonition">
<p class="admonition-title">Theorem: <em>Continuity of the M-P pseudo-inverse</em></p>
<p>The pseudo-inverse <span class="math notranslate nohighlight">\(K^\dagger\)</span> of <span class="math notranslate nohighlight">\(K\)</span> is continuous iff <span class="math notranslate nohighlight">\(\mathcal{R}(K)\)</span> is closed.</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Proof</p>
<p>For the proof, we refer to Thm 2.5 of <a class="reference external" href="https://mehrhardt.github.io/data/201803_lecture_notes_invprob.pdf">these lecture notes</a>.</p>
</div>
<hr class="docutils" />
<p>Let’s consider a concrete example.</p>
<div class="admonition-example-pseudo-inverse-of-a-bounded-operator admonition">
<p class="admonition-title">Example: Pseudo-inverse of a bounded operator</p>
<p>Consider the following forward operator</p>
<div class="math notranslate nohighlight">
\[Ku(x) = \int_{-\infty}^\infty k(x-y)u(y)\mathrm{d}y,\]</div>
<p>with <span class="math notranslate nohighlight">\(k, u \in L^{1}(\mathbb{R})\)</span>. <a class="reference external" href="https://en.wikipedia.org/wiki/Young%27s_convolution_inequality">Young’s inequality</a> tells us that <span class="math notranslate nohighlight">\(\|Ku\|_{L^1(\mathbb{R})} \leq \|k\|_{L^1(\mathbb{R})} \cdot \|u\|_{L^1(\mathbb{R})}\)</span> and hence that <span class="math notranslate nohighlight">\(K\)</span> is bounded.</p>
<p>We can alternatively represent <span class="math notranslate nohighlight">\(K\)</span> using <a class="reference external" href="https://en.wikipedia.org/wiki/Convolution_theorem">convolution theorem</a> as</p>
<div class="math notranslate nohighlight">
\[Ku(x) = F^{-1} (\widehat{k} \widehat{u}) (x),\]</div>
<p>where <span class="math notranslate nohighlight">\(F : L^1(\mathbb{R}) \rightarrow L^{\infty}(\mathbb{R})\)</span> denotes the <a class="reference external" href="https://en.wikipedia.org/wiki/Fourier_transform#On_L1">Fourier transform</a> and <span class="math notranslate nohighlight">\(\widehat{k} = Fk\)</span>, <span class="math notranslate nohighlight">\(\widehat{u} = Fu\)</span>.</p>
<p>This suggests defining the inverse of <span class="math notranslate nohighlight">\(K\)</span> as</p>
<div class="math notranslate nohighlight">
\[K^{-1}f = F^{-1} (\widehat{f} / \widehat{k}).\]</div>
<p>We note here that, by the <a class="reference external" href="https://en.wikipedia.org/wiki/Riemann%E2%80%93Lebesgue_lemma">Riemann–Lebesgue lemma</a>, the Fourier transform of <span class="math notranslate nohighlight">\(k\)</span> tends to zero at infinity. This means that the inverse of <span class="math notranslate nohighlight">\(K\)</span> is only well-defined when <span class="math notranslate nohighlight">\(\widehat{f}\)</span> decays faster than <span class="math notranslate nohighlight">\(\widehat{k}\)</span>. However, we can expect problems when <span class="math notranslate nohighlight">\(\widehat{k}\)</span> has roots as well.</p>
<p>As a concrete example, consider <span class="math notranslate nohighlight">\(k = \text{sinc}(x)\)</span> with <span class="math notranslate nohighlight">\(\widehat{k}(\xi) = H(\xi+1/2)-H(\xi-1/2)\)</span>. The forward operator then bandlimits the input. Can you think of a function in the null-space of <span class="math notranslate nohighlight">\(K\)</span>?</p>
<p>The pseudo-inverse may now be defined as</p>
<div class="math notranslate nohighlight">
\[K^{\dagger}f = F^{-1} B \widehat{f},\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}B\widehat{f}(\xi) = \begin{cases} \widehat{f}(\xi) &amp; |\xi| \leq 1/2 \\ 0 &amp; \text{otherwise}\end{cases}.\end{split}\]</div>
</div>
</div>
<div class="section" id="compact-operators">
<h2><span class="section-number">3.3. </span>Compact operators<a class="headerlink" href="#compact-operators" title="Permalink to this headline">¶</a></h2>
<p>An important subclass of the Bounded operators are the <a class="reference external" href="https://en.wikipedia.org/wiki/Compact_operator">compact operators</a>. They can be thought of as a natural generalisation of matrices to the infinite-dimensional setting. Hence, we can generalise the notions from the <a class="reference internal" href="discrete_ip_regularization.html"><span class="doc std std-doc">finite-dimensional setting</span></a> to the infinite-dimensional setting.</p>
<hr class="docutils" />
<p>There are a number of equivalent definitions of compact operators. We will use the following.</p>
<div class="important admonition">
<p class="admonition-title">Definitition: <em>Compact operator</em></p>
<p>An operator <span class="math notranslate nohighlight">\(K: \mathcal{U} \rightarrow \mathcal{F}\)</span> with <span class="math notranslate nohighlight">\(\mathcal{U},\mathcal{F}\)</span> Hilbert spaces, is called <em>compact</em> if it can be expressed as</p>
<div class="math notranslate nohighlight">
\[
K = \sum_{j=1}^{\infty} \sigma_j \langle \cdot, v_j\rangle_{\mathcal{U}}u_j,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\{v_i\}\)</span> and <span class="math notranslate nohighlight">\(\{u_i\}\)</span> are orthonormal bases of <span class="math notranslate nohighlight">\(\mathcal{N}(K)^\perp\)</span> and <span class="math notranslate nohighlight">\(\overline{\mathcal{R}(K)}\)</span> respectively and
<span class="math notranslate nohighlight">\(\{\sigma_i\}\)</span> is a null-sequence (i.e., <span class="math notranslate nohighlight">\(\lim_{i\rightarrow \infty} \sigma_i = 0\)</span>). We call <span class="math notranslate nohighlight">\(\{(u_i, v_i, \sigma_i)\}_{j=1}^\infty\)</span> the singular system of <span class="math notranslate nohighlight">\(K\)</span>. The singular system obeys</p>
<div class="math notranslate nohighlight">
\[Kv_j = \sigma_j u_j, \quad K^*u_j = \sigma_j v_j.\]</div>
</div>
<p>An important subclass of the compact operators are the <a class="reference external" href="https://en.wikipedia.org/wiki/Hilbert%E2%80%93Schmidt_integral_operator">Hilbert-Schmidt integral operators</a>, which can be written as</p>
<div class="math notranslate nohighlight">
\[Ku(x) = \int_{\Omega} k(x,y) u(y) \mathrm{d}y,\]</div>
<p>where <span class="math notranslate nohighlight">\(k: \Omega \times \Omega \rightarrow \mathbb{R}\)</span> is a Hilbert-Schmidt kernel obeying <span class="math notranslate nohighlight">\(\|k\|_{L^2(\Omega\times\Omega)} &lt; \infty\)</span> (i.e., it is square-integrable).</p>
<hr class="docutils" />
<p>The pseudo-inverse of a compact operator is defined analogously to the finite-dimensional setting:</p>
<div class="important admonition">
<p class="admonition-title">Definition: <em>Pseudo-inverse of a compact operator</em></p>
<p>The pseudo-inverse of a compact operator <span class="math notranslate nohighlight">\(K: \mathcal{U} \rightarrow \mathcal{F}\)</span> is expressed as</p>
<div class="math notranslate nohighlight">
\[
K^{\dagger} = \sum_{j=1}^{\infty} \sigma_j^{-1} \langle \cdot, u_j\rangle_{\mathcal{F}}v_j,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\{(u_i, v_i, \sigma_i)\}_{i=1}^\infty\)</span> is the singular system of <span class="math notranslate nohighlight">\(K\)</span></p>
</div>
<p>With this we can precisely state the Picard condition.</p>
<div class="important admonition">
<p class="admonition-title">Definition: <em>Picard condition</em></p>
<p>Given a compact operator <span class="math notranslate nohighlight">\(K: \mathcal{U} \rightarrow \mathcal{F}\)</span> and <span class="math notranslate nohighlight">\(f \in \mathcal{F}\)</span>, we have that <span class="math notranslate nohighlight">\(f \in \mathcal{R}(K) \cup \mathcal{R}(K)^\perp\)</span> iff</p>
<div class="math notranslate nohighlight" id="equation-picard">
<span class="eqno">(3.2)<a class="headerlink" href="#equation-picard" title="Permalink to this equation">¶</a></span>\[\sum_{j=1}^{\infty} \frac{|\langle f, u_j\rangle_{\mathcal{F}}|^2}{\sigma_j^2} &lt; \infty.\]</div>
</div>
<div class="important admonition">
<p class="admonition-title">Remark : <em>Degree of ill-posedness</em></p>
<p>For inverse problems with compact operators we can thus use the <em>Picard condition</em> to check if <a class="reference internal" href="#equation-minres">(3.1)</a> has a solution. Although this establishes existence and uniqueness through the pseudo-inverse, it does not guarantee stability as <span class="math notranslate nohighlight">\(\sigma_j \rightarrow 0\)</span>. If <span class="math notranslate nohighlight">\(\sigma_j\)</span> decays exponentially, we call the problem <em>severely ill-posed</em>, otherwise we call it <em>mildly ill-posed</em>.</p>
</div>
<hr class="docutils" />
<p>Let’s consider a few examples.</p>
<div class="admonition-example-a-sequence-operator admonition">
<p class="admonition-title">Example: A sequence operator</p>
<p>Consider the operator <span class="math notranslate nohighlight">\(K:\ell^2 \rightarrow \ell^2\)</span>, given by</p>
<div class="math notranslate nohighlight">
\[
	u = (u_1,u_2,...) \mapsto (0,u_2,\textstyle{\frac{1}{2}}u_3,...),
\]</div>
<p>i.e. we have an infinite matrix operator of the form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
K =
\left(\begin{matrix}
0 &amp;             &amp;             &amp;        &amp; \\
  &amp; 1           &amp;             &amp;        &amp; \\
  &amp;             &amp; \frac{1}{2} &amp;        &amp; \\
  &amp;             &amp;             &amp; \ddots &amp; \\
\end{matrix}\right)
\end{split}\]</div>
<p>The operator is obviously linear. To show that is bounded we’ll compute its norm:</p>
<div class="math notranslate nohighlight">
\[
\|K\| = \sup_{u \neq 0} \frac{\|Ku\|_{\ell^2}}{\|u\|_{\ell^2}} = 1.
\]</div>
<div class="dropdown note admonition">
<p class="admonition-title">derivation</p>
<p>We can fix <span class="math notranslate nohighlight">\(\|u\|_{\ell_2} = 1\)</span> and verify that the maximum is obtained for <span class="math notranslate nohighlight">\(u = (0,1,0,\ldots)\)</span> leading to <span class="math notranslate nohighlight">\(\|K\| = 1\)</span>.</p>
</div>
<p>To show that the operator is compact, we explicitly construct its singular system, giving:
<span class="math notranslate nohighlight">\(u_i = v_i = e_i\)</span> with <span class="math notranslate nohighlight">\(e_i\)</span> the <span class="math notranslate nohighlight">\(i^{\text{th}}\)</span> canonical basis vector and <span class="math notranslate nohighlight">\(\sigma_1 = 0\)</span>, <span class="math notranslate nohighlight">\(\sigma_{i} = (i-1)^{-1}\)</span> for <span class="math notranslate nohighlight">\(i &gt; 1\)</span>.</p>
<div class="dropdown note admonition">
<p class="admonition-title">derivation</p>
<p>Indeed, it is easily verified that <span class="math notranslate nohighlight">\(Ke_i = \sigma_i e_i\)</span>.</p>
</div>
<p>The pseudo inverse is then defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
K^{\dagger} =
\left(\begin{matrix}
0 &amp;             &amp;             &amp;        &amp; \\
  &amp; 1           &amp;             &amp;        &amp; \\
  &amp;             &amp; 2           &amp;        &amp; \\
  &amp;             &amp;             &amp; \ddots &amp; \\
\end{matrix}\right)
\end{split}\]</div>
<p>This immediately shows that <span class="math notranslate nohighlight">\(K^\dagger\)</span> is not bounded. Now consider obtaining a solution for <span class="math notranslate nohighlight">\(f = (1,1,\textstyle{\frac{1}{2}}, \textstyle{\frac{1}{3}},\ldots)\)</span>. Applying the pseudo-inverse would yield <span class="math notranslate nohighlight">\(K^\dagger f = (0,1, 1, \ldots)\)</span> which is not in <span class="math notranslate nohighlight">\(\ell_2\)</span>. Indeed, we can show that <span class="math notranslate nohighlight">\(f \not\in \mathcal{R}(K) \oplus \mathcal{R}(K)^\perp\)</span>. The problem here is that the range of <span class="math notranslate nohighlight">\(K\)</span> is not closed.</p>
</div>
<hr class="docutils" />
<div class="admonition-example-differentiation admonition">
<p class="admonition-title">Example: Differentiation</p>
<p>Consider <span class="math notranslate nohighlight">\(K:L_2([0,1]) \rightarrow L_2([0,1])\)</span> defined as</p>
<div class="math notranslate nohighlight">
\[
Ku(x) = \int_0^x u(y)\mathrm{d}y.
\]</div>
<p>Given <span class="math notranslate nohighlight">\(f(x) = Ku(x)\)</span> we would naively let <span class="math notranslate nohighlight">\(u(x) = f'(x)\)</span>. Let’s analyse this in more detail.</p>
<p>The operator can be expressed as</p>
<div class="math notranslate nohighlight">
\[
Ku(x) = \int_0^1 k(x,y)u(y)\mathrm{d}y,
\]</div>
<p>with <span class="math notranslate nohighlight">\(k(x,y) = H(x-y)\)</span>, where <span class="math notranslate nohighlight">\(H\)</span> denotes the Heaviside stepfunction. The operator is compact because <span class="math notranslate nohighlight">\(k\)</span> is square integrable.</p>
<div class="dropdown note admonition">
<p class="admonition-title">derivation</p>
<p>Indeed, we have</p>
<div class="math notranslate nohighlight">
\[\int_0^1 \int_0^1 |k(x,y)|^2 \mathrm{d}x\mathrm{d}y = \textstyle{\frac{1}{2}}.\]</div>
<p>We conclude that <span class="math notranslate nohighlight">\(k\)</span> is a Hilbert-Schmidt kernel and hence that <span class="math notranslate nohighlight">\(K\)</span> is compact.</p>
</div>
<p>The adjoint is found to be</p>
<div class="math notranslate nohighlight">
\[
K^*f(y) = \int_0^1 k(x,y) f(x)\mathrm{d}x = \int_y^1 f(x)\mathrm{d}x.
\]</div>
<div class="dropdown note admonition">
<p class="admonition-title">derivation</p>
<p>Using the definition we find</p>
<div class="math notranslate nohighlight">
\[K^*f(y) = \int_0^1 k(x,y)f(x)\mathrm{d}x = \int_y^1 f(x)\mathrm{d}x.\]</div>
</div>
<p>The singular system is given by</p>
<div class="math notranslate nohighlight">
\[
\sigma_k = ((k+1/2)\pi)^{-1},
\]</div>
<div class="math notranslate nohighlight">
\[
u_k(x) = \sqrt{2}\sin(\sigma_k^{-1} x), 
\]</div>
<div class="math notranslate nohighlight">
\[
\quad v_k(x) = \sqrt{2}\cos(\sigma_k^{-1} x).
\]</div>
<div class="dropdown note admonition">
<p class="admonition-title">derivation</p>
<p>To derive the singular system, we first need to compute the eigenpairs <span class="math notranslate nohighlight">\((\lambda_k, v_k)\)</span> of <span class="math notranslate nohighlight">\(K^*K\)</span>. The singular system is then given by <span class="math notranslate nohighlight">\((\sqrt{\lambda_k}, (\sqrt{\lambda_k})^{-1}Kv_k, v_k)\)</span>.</p>
<p>We find</p>
<div class="math notranslate nohighlight">
\[
K^*Kv(y) = \int_y^1 \int_0^x v(z) \, \mathrm{d}z\mathrm{d}x = \lambda v(y).
\]</div>
<p>At <span class="math notranslate nohighlight">\(y = 1\)</span> this yields <span class="math notranslate nohighlight">\(v(1) = 0\)</span>. Differentiating, we find</p>
<div class="math notranslate nohighlight">
\[
\lambda v'(y) = -\int_0^y v(z)\mathrm{d}z,
\]</div>
<p>which yields <span class="math notranslate nohighlight">\(v'(0) = 0\)</span>. Differentiating once again, we find</p>
<div class="math notranslate nohighlight">
\[
\lambda v''(x) = -v(x).
\]</div>
<p>The general solution to this differential equation is</p>
<div class="math notranslate nohighlight">
\[
v(x) = a\sin(x/\sqrt{\lambda}) + b\cos(x/\sqrt{\lambda}).
\]</div>
<p>Using the boundary condition at <span class="math notranslate nohighlight">\(x = 0\)</span> we find that <span class="math notranslate nohighlight">\(a = 0\)</span>. Using the boundary condition at <span class="math notranslate nohighlight">\(x = 1\)</span> we get</p>
<div class="math notranslate nohighlight">
\[
b\cos(1/\sqrt{\lambda}) = 0,
\]</div>
<p>which yields <span class="math notranslate nohighlight">\(\lambda_k = 1/((k + 1/2)^2\pi^2)\)</span>, <span class="math notranslate nohighlight">\(k = 0, 1, \ldots\)</span>. We choose <span class="math notranslate nohighlight">\(b\)</span> to normalize <span class="math notranslate nohighlight">\(\|v_k\| = 1\)</span>.</p>
</div>
<p>The operator can thus be expressed as</p>
<div class="math notranslate nohighlight">
\[
Ku(x) = \sum_{k=0}^\infty \frac{\langle u, v_k\rangle}{(k+1/2)\pi} u_k(x),
\]</div>
<p>and the pseudo-inverse by</p>
<div class="math notranslate nohighlight">
\[
K^{\dagger}f(x) = \pi\sum_{k=0}^\infty (k+1/2)\langle f, u_k\rangle v_k(x).
\]</div>
<p>We can now study the ill-posedness of the problem by looking at the Picard condition</p>
<div class="math notranslate nohighlight">
\[
\|K^\dagger f\|^2 = \pi^2\sum_{k=0}^\infty f_k^2 (k+1/2)^2,
\]</div>
<p>where <span class="math notranslate nohighlight">\(f_k = \langle f, u_k\rangle\)</span> are the (generalized) Fourier coefficients of <span class="math notranslate nohighlight">\(f\)</span>.</p>
<p>For this infinite sum to converge, we need strong requirements on <span class="math notranslate nohighlight">\(f_k\)</span>; for example <span class="math notranslate nohighlight">\(f_k = 1/k\)</span> does not suffice to make the sum converge. This is quite surprising since such an <span class="math notranslate nohighlight">\(f\)</span> is square-integrable. It turns out we need <span class="math notranslate nohighlight">\(f_k = \mathcal{O}(1/k^2)\)</span> to satisfy the Picard condition. Effectively this means that <span class="math notranslate nohighlight">\(f'\)</span> needs to be square integrable. This makes sense since we saw earlier that <span class="math notranslate nohighlight">\(u(x) = f'(x)\)</span> is the solution to <span class="math notranslate nohighlight">\(Ku = f\)</span>.</p>
</div>
</div>
<div class="section" id="regularisation">
<h2><span class="section-number">3.4. </span>Regularisation<a class="headerlink" href="#regularisation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="truncation-and-tikhonov-regularisation">
<h3><span class="section-number">3.4.1. </span>Truncation and Tikhonov regularisation<a class="headerlink" href="#truncation-and-tikhonov-regularisation" title="Permalink to this headline">¶</a></h3>
<p>In the previous section we saw that the pseudo-inverse of a compact operator is not bounded (continuous) in general. To counter this, we introduce the regularized pseudo-inverse:</p>
<div class="math notranslate nohighlight">
\[
K_{\alpha}^{\dagger}f = \sum_{k=0}^{\infty} g_{\alpha}(\sigma_k) \langle f, u_k\rangle v_k,
\]</div>
<p>where <span class="math notranslate nohighlight">\(g_{\alpha}\)</span> determines the type of regularization used. For Tikhonov regularisation we let</p>
<div class="math notranslate nohighlight">
\[
g_{\alpha}(s) = \frac{s}{s^2 + \alpha}= \frac{1}{s + \alpha/s}.
\]</div>
<p>For a truncated SVD we let</p>
<div class="math notranslate nohighlight">
\[\begin{split}
g_{\alpha}(s) = \begin{cases} s^{-1} &amp; \text{if}\, s &gt; \alpha \\ 0 &amp; \text{otherwise} \end{cases}.
\end{split}\]</div>
<p>We can show that the regularized pseudo inverse, <span class="math notranslate nohighlight">\(K_{\alpha}^{\dagger}\)</span>, is bounded for <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span> and converges to <span class="math notranslate nohighlight">\(K^\dagger\)</span> as <span class="math notranslate nohighlight">\(\alpha \rightarrow 0\)</span>.</p>
<p>Given noisy data <span class="math notranslate nohighlight">\(f^{\delta} = f + e\)</span> with <span class="math notranslate nohighlight">\(\|e\| \leq \delta\)</span>, we can now study the effect of regularisation by studying the error. The total error is now given by</p>
<div class="math notranslate nohighlight">
\[
\|K_{\alpha}^\dagger f^\delta - K^\dagger f\|_{\mathcal{U}} \leq \|K_{\alpha}^\dagger f - K^\dagger f\|_{\mathcal{U}} + \|K_{\alpha}^\dagger(f^\delta - f)\|_{\mathcal{U}},
\]</div>
<p>in which we recognise the <em>bias</em> and <em>variance</em> contributions. Note that we may bound this even further as</p>
<div class="math notranslate nohighlight">
\[\|K_{\alpha}^\dagger f^\delta - K^\dagger f\|_{\mathcal{U}} \leq \|K_{\alpha}^\dagger - K^\dagger\| \|f\|_{\mathcal{F}} + \delta \|K_{\alpha}^\dagger\|.\]</div>
<p>Alternatively, we may express the error in terms of the minimum-norm solution <span class="math notranslate nohighlight">\(u = K^{\dagger} Ku = K^\dagger f\)</span> as</p>
<div class="math notranslate nohighlight">
\[\|K_{\alpha}^\dagger f^\delta - K^\dagger f\|_{\mathcal{U}} \leq \|I - K_{\alpha}^\dagger K\| \|u\|_{\mathcal{U}} +  \delta \|K_{\alpha}^\dagger\|.\]</div>
<p>Such upper bounds may be useful to study asymptotic properties of the problem but may be too loose to be used in practice. In that case a more detailed analysis incorporating the type of noise and the class of images <span class="math notranslate nohighlight">\(u\)</span> that we are interested in is needed.</p>
<div class="admonition-example-differentiation admonition">
<p class="admonition-title">Example: Differentiation</p>
<p>Consider adding Tikhonov regularisation to stabilise the differentiation problem.
Take measurements <span class="math notranslate nohighlight">\(f^{\delta} = Ku + \delta\sin(\delta^{-1}x)\)</span>, where <span class="math notranslate nohighlight">\(\delta = \sigma_k\)</span> for some <span class="math notranslate nohighlight">\(k\)</span>. The error <span class="math notranslate nohighlight">\(K^\dagger f^{\delta} - u\)</span> is then given by</p>
<div class="math notranslate nohighlight">
\[
K^\dagger K u - u + \delta K^{\dagger}\sin(\delta^{-1}\cdot).
\]</div>
<p>Because <span class="math notranslate nohighlight">\(\delta^{-1} = \sigma_k\)</span> and <span class="math notranslate nohighlight">\(\sin(\sigma_k^{-1}x)\)</span> is a singular vector of <span class="math notranslate nohighlight">\(K\)</span> (or because the pseudo-inverse acts as taking a derivative), this simplifies to</p>
<div class="math notranslate nohighlight">
\[
\cos(\sigma_k^{-1}x).
\]</div>
<p>Thus, the reconstruction error does not go to zero as <span class="math notranslate nohighlight">\(\delta\downarrow 0\)</span>, even though the error in the data does.</p>
<p>The eigenvalues of <span class="math notranslate nohighlight">\(K_{\alpha}^\dagger K\)</span> are given by <span class="math notranslate nohighlight">\((1 + \alpha \sigma_k^{-2})^{-1}\)</span>, with <span class="math notranslate nohighlight">\(\sigma_k = (\pi(k + 1/2))^{-1}\)</span>. The bias is thus given by</p>
<div class="math notranslate nohighlight">
\[
\|I - K_{\alpha}^\dagger K\| = \max_{k} \left|1 - (1 + \alpha \sigma_{k}^{-2})^{-1}\right|.
\]</div>
<p>Likewise, the variance is given by</p>
<div class="math notranslate nohighlight">
\[
\|K_{\alpha}^\dagger\| = \max_{k}\frac{1}{\sigma_k + \alpha \sigma_{k}^{-1}}.
\]</div>
</div>
</div>
<div class="section" id="generalised-tikhonov-regularisation">
<h3><span class="section-number">3.4.2. </span>Generalised Tikhonov regularisation<a class="headerlink" href="#generalised-tikhonov-regularisation" title="Permalink to this headline">¶</a></h3>
<p>We have seen in the finite-dimensional setting that Tikhonov regularization may be defined through a variational problem:</p>
<div class="math notranslate nohighlight">
\[
\min_{u} \|Ku - f\|^2_{\mathcal{F}} + \alpha \|u\|^2_{\mathcal{U}}.
\]</div>
<p>It turns out we can do the same in the infinite-dimensional setting. Indeed, we can show that the corresponding normal equations are given by</p>
<div class="math notranslate nohighlight">
\[(K^*\!K + \alpha I)u = K^*f.\]</div>
<p>Generalised Tikhonov regularisation is defined in a similar manner through the variation problem</p>
<div class="math notranslate nohighlight">
\[
\min_{u} \|Ku - f\|^2_{\mathcal{F}} + \alpha \|Lu\|^2_{\mathcal{V}},
\]</div>
<p>where <span class="math notranslate nohighlight">\(L:\mathcal{U}\rightarrow \mathcal{V}\)</span> is a (not necessarily bounded) linear operator. The corresponding normal equations can be shown to be</p>
<div class="math notranslate nohighlight">
\[(K^*\!K + \alpha L^*\!L)u = K^*f.\]</div>
<p>We can expect a unique solution when the intersection of the kernels of <span class="math notranslate nohighlight">\(K\)</span> and <span class="math notranslate nohighlight">\(L\)</span> is empty. In many applications, <span class="math notranslate nohighlight">\(L\)</span> is a differential operator. This can be used to impose smoothness on the solution.</p>
</div>
<div class="section" id="parameter-choice-strategies">
<h3><span class="section-number">3.4.3. </span>Parameter-choice strategies<a class="headerlink" href="#parameter-choice-strategies" title="Permalink to this headline">¶</a></h3>
<p>Given a regularisation strategy with parameter <span class="math notranslate nohighlight">\(\alpha\)</span>, we need to pick <span class="math notranslate nohighlight">\(\alpha\)</span>. As mentioned earlier, we need to pick <span class="math notranslate nohighlight">\(\alpha\)</span> to optimally balance between the bias and variance in the error. To highlight the basic flavours of parameter-choice strategies, we give three examples.</p>
<div class="admonition-example-a-priori-rules admonition">
<p class="admonition-title">Example: <em>A-priori rules</em></p>
<p>Assuming that we know the noise level <span class="math notranslate nohighlight">\(\delta\)</span>, we can define a parameter-choice rule <span class="math notranslate nohighlight">\(\alpha(\delta)\)</span>. We call such a rule <em>convergent</em> iff</p>
<div class="math notranslate nohighlight">
\[\lim_{\delta\rightarrow 0} \alpha(\delta) = 0, \quad \lim_{\delta\rightarrow 0} \delta \|K_{\alpha(\delta)}^\dagger\| = 0.\]</div>
<p>With these requirements, we can easily show that the error <span class="math notranslate nohighlight">\(\|K_{\alpha}^\dagger f^{\delta} - K^\dagger f\|_{\mathcal{U}} \rightarrow 0\)</span> as <span class="math notranslate nohighlight">\(\delta\rightarrow 0\)</span>.</p>
<p>Such parameter-choice rules are nice in theory, but hard to design in practice. One is sometimes interested in the <em>convergence rate</em>, which aims to bound the bias in terms of <span class="math notranslate nohighlight">\(\delta^\nu\)</span> for some <span class="math notranslate nohighlight">\(0 &lt; \nu &lt; 1\)</span>.</p>
</div>
<div class="admonition-example-the-discrepancy-principle admonition">
<p class="admonition-title">Example: <em>The discrepancy principle</em></p>
<p>Morozov’s discrepancy principle chooses <span class="math notranslate nohighlight">\(\alpha\)</span> such that</p>
<div class="math notranslate nohighlight">
\[\|KK_{\alpha}^\dagger f^{\delta} - f^{\delta}\|_{\mathcal{F}} \leq \eta \delta,\]</div>
<p>with <span class="math notranslate nohighlight">\(\eta &gt; 1\)</span> a fixed parameter. This can be interpreted as finding an <span class="math notranslate nohighlight">\(\alpha\)</span> for which the solution fits the data in accordance with the noise level. Note, however, that such an <span class="math notranslate nohighlight">\(\alpha\)</span> may not exist if (a significant part of) <span class="math notranslate nohighlight">\(f^\delta\)</span> lies in the kernel of <span class="math notranslate nohighlight">\(K^*\)</span>. This is an example of an <em>a-posterior</em> parameter selection rule, as it depends on both <span class="math notranslate nohighlight">\(\delta\)</span> and <span class="math notranslate nohighlight">\(f^\delta\)</span>.</p>
</div>
<div class="admonition-example-the-l-curve-method admonition">
<p class="admonition-title">Example: <em>The L-curve method</em></p>
<p>Here, we choose <span class="math notranslate nohighlight">\(\alpha\)</span> via</p>
<div class="math notranslate nohighlight">
\[\min_{\alpha &gt; 0} \|K_{\alpha}^\dagger f^\delta\|_{\mathcal{U}} \|KK_{\alpha}^\dagger f^\delta - f^\delta\|_{\mathcal{F}}.\]</div>
<p>The name stems from the fact that the optimal <span class="math notranslate nohighlight">\(\alpha\)</span> typically resides at the corner of the curve <span class="math notranslate nohighlight">\((\|K_{\alpha}^\dagger f^\delta\|_{\mathcal{U}}, \|KK_{\alpha}^\dagger f^\delta - f^\delta\|_{\mathcal{F}})\)</span>.</p>
<p>This rule has the practical advantage that no knowledge of the noise level is required. Such a rule is called a <em>heuristic</em> method. Unfortunately, it is not a convergent rule.</p>
</div>
</div>
</div>
<div class="section" id="exercises">
<h2><span class="section-number">3.5. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<div class="section" id="convolution">
<h3><span class="section-number">3.5.1. </span>Convolution<a class="headerlink" href="#convolution" title="Permalink to this headline">¶</a></h3>
<p>Consider the example in section <a class="reference external" href="ip_function_spaces.html#bounded-operators-on-hilbert-spaces">3.2</a> with <span class="math notranslate nohighlight">\(k(x) = \exp(-|x|)\)</span>.</p>
<ul class="simple">
<li><p>Is the inverse problem ill-posed?</p></li>
<li><p>For which functions <span class="math notranslate nohighlight">\(f\)</span> is the inverse well-defined?</p></li>
</ul>
<div class="hint dropdown admonition">
<p class="admonition-title">Answer</p>
<p>Here we have <span class="math notranslate nohighlight">\(\widehat{k}(\xi) = \sqrt{2/\pi} (1 + \xi^2)^{-1}\)</span> (cf. <a class="reference external" href="https://en.wikipedia.org/wiki/Fourier_transform#Tables_of_important_Fourier_transforms">this table</a>). For the inverse we then need <span class="math notranslate nohighlight">\((1 + \xi^2)\widehat{f}(\xi)\)</span> to be bounded. We conclude that the inverse problem is ill-posed since a solution will not exist for all <span class="math notranslate nohighlight">\(f\)</span>. Moreover, we can expect amplification of the noise if it has non-zero Fourier coefficients for large <span class="math notranslate nohighlight">\(|\xi|\)</span>.</p>
<p>We can only define the inverse for functions <span class="math notranslate nohighlight">\(f\)</span> whose Fourier transform decays rapidly in the sense that <span class="math notranslate nohighlight">\(\lim_{|\xi|\rightarrow} (1 + \xi^2)\widehat{f}(\xi) &lt; \infty\)</span>. Examples are <span class="math notranslate nohighlight">\(f(x) = e^{-ax^2}\)</span> or <span class="math notranslate nohighlight">\(f(x) = \text{sinc}(ax)\)</span> for any <span class="math notranslate nohighlight">\(a &gt; 0\)</span>. We may formalise this by introducing <a class="reference external" href="https://en.wikipedia.org/wiki/Schwartz_space">Schwartz functions</a> but this is beyond the scope of this course.</p>
</div>
</div>
<div class="section" id="differentiation">
<h3><span class="section-number">3.5.2. </span>Differentiation<a class="headerlink" href="#differentiation" title="Permalink to this headline">¶</a></h3>
<p>Consider the forward operator</p>
<div class="math notranslate nohighlight">
\[
Ku(x) = \int_0^x u(y)\mathrm{d}y.
\]</div>
<p>We’ve seen in the example that the inverse problem is ill-posed. Consider the regularised least-squares problem</p>
<div class="math notranslate nohighlight">
\[\min_{u} \|Ku - f^\delta\|^2 + \alpha \|u'\|^2,\]</div>
<p>with <span class="math notranslate nohighlight">\(\|\cdot\|\)</span> denoting the <span class="math notranslate nohighlight">\(L^2([0,1])\)</span>-norm. In this exercise we are going to analyse how this type of regularisation addresses the ill-posedness by looking at the variance term.</p>
<ul class="simple">
<li><p>Show that the corresponding regularised pseudo-inverse is given by</p></li>
</ul>
<div class="math notranslate nohighlight">
\[K_{\alpha}^\dagger = \sum_{k=0}^\infty \frac{\sigma_k \langle u_k, \cdot \rangle}{\sigma_k^2 + \alpha \sigma_k^{-2}} v_k(x),\]</div>
<p>where <span class="math notranslate nohighlight">\(\{(u_k,v_k,\sigma_k)\}_{k=0}^\infty\)</span> denotes the singular system of <span class="math notranslate nohighlight">\(K\)</span>.</p>
<ul class="simple">
<li><p>Take <span class="math notranslate nohighlight">\(f^\delta(x) = f(x) + \sigma_k \sqrt{2}\sin(\sigma_k^{-1}x)\)</span> with <span class="math notranslate nohighlight">\(\sigma_k\)</span> a singular value of <span class="math notranslate nohighlight">\(K\)</span> and show that for <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span> the variance <span class="math notranslate nohighlight">\(\|K_{\alpha}^\dagger (f^\delta - f)\| \rightarrow 0\)</span> as <span class="math notranslate nohighlight">\(k \rightarrow 0\)</span></p></li>
</ul>
<div class="hint dropdown admonition">
<p class="admonition-title">Answer</p>
<p>We will use the SVD of <span class="math notranslate nohighlight">\(K\)</span> to express the solution and derive a least-squares problem for the coefficients of <span class="math notranslate nohighlight">\(u\)</span> using the orthonormality of the singular vectors.</p>
<p>The right singular vectors are given by <span class="math notranslate nohighlight">\(v_k(x) = \sqrt{2}\cos\left(\sigma_k^{-1} x\right)\)</span> with <span class="math notranslate nohighlight">\(\sigma_k = (\pi(k+1/2))^{-1}\)</span>. Since these constitute an orthonormal basis for the orthogonal complement of the kernel of <span class="math notranslate nohighlight">\(K\)</span> we can express <span class="math notranslate nohighlight">\(u\)</span> as</p>
<div class="math notranslate nohighlight">
\[u(x) = \sum_{k=0}^\infty a_k v_k(x) + w, \]</div>
<p>with <span class="math notranslate nohighlight">\(Kw = 0\)</span>. We’ll ignore <span class="math notranslate nohighlight">\(w\)</span> for the time being and assume without proof that <span class="math notranslate nohighlight">\(\{v_k\}_{k=0}^\infty\)</span> is an orthonormal basis for <span class="math notranslate nohighlight">\(L^2([0,1])\)</span>.</p>
<p>We can now express the least-squares problem in terms of the coefficients <span class="math notranslate nohighlight">\(a_k\)</span> First note that</p>
<div class="math notranslate nohighlight">
\[u'(x) = -\sum_{k=0}^\infty \sigma_k^{-1}a_k u_k(x),\]</div>
<p>with <span class="math notranslate nohighlight">\(u_k(x)\)</span> denoting the left singular vectors <span class="math notranslate nohighlight">\(u_k(x) = \sqrt{2}\sin\left(\sigma_k^{-1} x\right)\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[\|u'\|^2 = \sum_{k=0}^\infty \frac{a_k^2}{\sigma_k^2},\]</div>
<p>and using the fact that <span class="math notranslate nohighlight">\(Kv_k = \sigma_k u_k\)</span>:</p>
<div class="math notranslate nohighlight">
\[\|Ku - f\|^2 = \sum_{k=0}^\infty (\sigma_k a_k - f_k)^2,\]</div>
<p>with <span class="math notranslate nohighlight">\(f_k = \langle u_k, f\rangle\)</span>. The normal equations are now given by</p>
<div class="math notranslate nohighlight">
\[\left(\sigma_k^2 + \alpha \sigma_k^{-2}\right)a_k = \sigma_k f_k,\]</div>
<p>yielding</p>
<div class="math notranslate nohighlight">
\[u(x) = \sum_{k=0}^\infty \frac{\sigma_k \langle u_k, f \rangle}{\sigma_k^2 + \alpha \sigma_k^{-2}} v_k(x).\]</div>
<p>We can now study what happens to the variance term <span class="math notranslate nohighlight">\(\|K_{\alpha}^\dagger e\|\)</span> with <span class="math notranslate nohighlight">\(e_k(x) = \sigma_k \sin(x/\sigma_k)\)</span>. First note that (by orthogonality)</p>
<div class="math notranslate nohighlight">
\[K_{\alpha}^\dagger e_k(x) = \frac{\sigma_k^2}{\sigma_k^2 + \alpha \sigma_k^{-2}} \cos(\sigma_k^{-1} x).\]</div>
<p>We see, as before, that for <span class="math notranslate nohighlight">\(\alpha = 0\)</span> the variance is constant as <span class="math notranslate nohighlight">\(k\rightarrow \infty\)</span> (i.e, noise level going to zero). For <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span>, however, we see that the variance tends to zero.</p>
</div>
</div>
<div class="section" id="convergent-regularisation">
<h3><span class="section-number">3.5.3. </span>Convergent regularisation<a class="headerlink" href="#convergent-regularisation" title="Permalink to this headline">¶</a></h3>
<p>Consider the regularised pseudo-inverse</p>
<div class="math notranslate nohighlight">
\[K^\dagger_{\alpha} = \sum_{k=0}^\infty g_{\alpha}(\sigma_k) \langle u_k,\cdot\rangle v_k,\]</div>
<p>and let <span class="math notranslate nohighlight">\(f^\delta = f + e\)</span> with <span class="math notranslate nohighlight">\(\|e\|_{\mathcal{F}} \leq \delta\)</span>. We are going to study the convergence of this method, i.e., how fast <span class="math notranslate nohighlight">\(\|K_{\alpha}^\dagger f^\delta - K^\dagger f\| \rightarrow 0\)</span>. You may assume that <span class="math notranslate nohighlight">\(f \in \mathcal{R}(K)\)</span> and that <span class="math notranslate nohighlight">\(f^\delta\)</span> obeys the Picard condition. We further let</p>
<div class="math notranslate nohighlight">
\[\begin{split}g_{\alpha}(s) = \begin{cases} s^{-1} &amp; s &gt; \alpha \\ 0 &amp; s \leq \alpha \end{cases}.\end{split}\]</div>
<p>and <span class="math notranslate nohighlight">\(\alpha(\delta) = \sqrt{\delta}\)</span>.</p>
<ul class="simple">
<li><p>Show that the total error <span class="math notranslate nohighlight">\(\|K_{\alpha}^\dagger f^\delta - K^\dagger f\|\)</span> converges to zero as <span class="math notranslate nohighlight">\(\delta \rightarrow 0\)</span>.</p></li>
<li><p>Show that the variance term converges to zero as <span class="math notranslate nohighlight">\(\delta\rightarrow 0\)</span> with rate <span class="math notranslate nohighlight">\(1/2\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\|K_{\alpha}^\dagger e\|_{\mathcal{U}} = \mathcal{O}(\sqrt{\delta}).\]</div>
<p>Under additional assumptions on the minimum-norm solution we can provide a faster convergence rate in. Assume that for a given <span class="math notranslate nohighlight">\(\mu &gt; 0\)</span> there exists a <span class="math notranslate nohighlight">\(w\)</span> such that <span class="math notranslate nohighlight">\(K^\dagger f = (K^*K)^{\mu} w\)</span>.</p>
<ul class="simple">
<li><p>Show that this condition implies that (i.e., more regularity of <span class="math notranslate nohighlight">\(f\)</span>)</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\sum_{k=0}^{\infty} \frac{|\langle f,u_k\rangle|^2}{\sigma_k^{2(1 + 2\mu)}} &lt; \infty.\]</div>
<ul class="simple">
<li><p>Show that</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\|K^\dagger_{\alpha} f - K^\dagger f\| = \mathcal{O}(\delta^{\mu}).\]</div>
<div class="dropdown hint admonition">
<p class="admonition-title">Answer</p>
<ul class="simple">
<li><p>We split the error in its bias and variance terms and consider each separately. The bias term is given by <span class="math notranslate nohighlight">\(\|(K^\dagger - K_{\alpha}^\dagger)f\|\)</span>, which we express as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\|(K^\dagger - K_{\alpha}^\dagger)f\|^2 = \sum_{k=0}^\infty (\sigma_k^{-1} - g_{\alpha}(\sigma_k))^2 |\langle f, u_k\rangle|^2.\]</div>
<p>We can split off a factor <span class="math notranslate nohighlight">\(\sigma_k^{-1}\)</span> and bound as</p>
<div class="math notranslate nohighlight">
\[\|(K^\dagger - K_{\alpha}^\dagger)f\|^2 \leq \sup_k (1 - \sigma_k g_{\alpha}(\sigma_k))^2 \sum_{k=0}^\infty  \frac{|\langle f, u_k\rangle|^2}{\sigma_k^2}.\]</div>
<p>Since <span class="math notranslate nohighlight">\(f\)</span> obeys the Picard condition, the second term is finite. The first term can be bounded because <span class="math notranslate nohighlight">\(0 &lt; sg_{\alpha}(s) \leq 1\)</span>. Moreover, we see that at <span class="math notranslate nohighlight">\(\alpha = 0\)</span> we have <span class="math notranslate nohighlight">\((1 - \sigma_k g_{\alpha}(\sigma_k))^2 = 0\)</span>. We conclude that the bias converges to zero as <span class="math notranslate nohighlight">\(\delta \rightarrow 0\)</span>. We do not get a rate, however.</p>
<p>The variance term can be shown to converge to zero as well, as done below.</p>
<ul class="simple">
<li><p>We find</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\|K^\dagger_{\alpha} e\| \leq \|K^\dagger_{\alpha}\| \delta,\]</div>
<p>with <span class="math notranslate nohighlight">\(\|K^\dagger_{\alpha(\delta)}\| = \sup_k |g_{\alpha(\delta)}(\sigma_k)|\)</span>. To bound this, we observe that <span class="math notranslate nohighlight">\(|g_{\alpha}(s)| \leq \alpha^{-1}\)</span>. Setting <span class="math notranslate nohighlight">\(\alpha = \sqrt{\delta}\)</span> we get the desired result.</p>
<ul class="simple">
<li><p>Start from</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\sum_{k=0}^{\infty} \frac{|\langle f,u_k\rangle|^2}{\sigma_k^{2 + 4\mu}}\]</div>
<p>and substitute <span class="math notranslate nohighlight">\(f = K(K^*K)^{\mu} w = \sum_{k=0}^\infty \sigma_k^{2\mu+1}\langle w,u_k\rangle u_k\)</span> to get</p>
<div class="math notranslate nohighlight">
\[\sum_{k=0}^{\infty} \sigma_k^{2 + 4\mu}\frac{|\langle w,u_k\rangle|^2}{\sigma_k^{2 + 4\mu}} = \|w\|^2 &lt; \infty.\]</div>
<ul class="simple">
<li><p>Starting again from the bias term, we can factor out an additional <span class="math notranslate nohighlight">\(\sigma^{4\mu}\)</span> term. We then use that <span class="math notranslate nohighlight">\(s^{2\mu}(1 - sg_{\alpha}(s)) \leq \alpha^{2\mu} \)</span> to find the desired result.</p></li>
</ul>
</div>
</div>
<div class="section" id="convolution-through-the-heat-equation">
<h3><span class="section-number">3.5.4. </span>Convolution through the heat equation<a class="headerlink" href="#convolution-through-the-heat-equation" title="Permalink to this headline">¶</a></h3>
<p>In this exercise we’ll explore the relation between the heat equation and convolution with a Gaussian kernel. Define the forward problem <span class="math notranslate nohighlight">\(f = Ku\)</span> via the initial-value problem</p>
<div class="math notranslate nohighlight">
\[
v_t = v_{xx}, \quad v(0,x) = u(x), \quad f(x) = v(T,x).
\]</div>
<ul class="simple">
<li><p>Verify that the solution to the heat equation is given by</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
v(t,x) = \int_{\mathbb{R}} u(x') g_t(x - x')\mathrm{d}x',
\]</div>
<p>where <span class="math notranslate nohighlight">\(g_t(x)\)</span> is the <em>heat-kernel</em>:</p>
<div class="math notranslate nohighlight">
\[
g_t(x) = \frac{1}{2\sqrt{\pi t}}\exp(-(x/2)^2/t).
\]</div>
<p>You may assume that <span class="math notranslate nohighlight">\(u\)</span> is sufficiently regular and use that <span class="math notranslate nohighlight">\(g_t(x)\)</span> converges (in the sense of distributions) to <span class="math notranslate nohighlight">\(\delta(x)\)</span> as <span class="math notranslate nohighlight">\(t \downarrow 0\)</span>.</p>
<ul class="simple">
<li><p>Show that we can thus express the forward operator as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
Ku(x) = \frac{1}{2\sqrt{\pi T}}\int_{\mathbb{R}} u(x') \exp(-(x - x')^2/(4T)) \mathrm{d}x'.
\]</div>
<ul class="simple">
<li><p>Show that the operator may be expressed as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
Ku = F^{-1}((Fu)\cdot(Fg_T)),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\cdot\)</span> denotes point-wise multiplication and <span class="math notranslate nohighlight">\(F\)</span> denotes the <a class="reference external" href="https://en.wikipedia.org/wiki/Fourier_transform">Fourier transform</a>.</p>
<ul class="simple">
<li><p>Express the inverse of <span class="math notranslate nohighlight">\(K\)</span> as multiplication with a filter <span class="math notranslate nohighlight">\(\widehat{h}\)</span> (in the Fourier domain). How does ill-posed manifest itself here and how does it depend on <span class="math notranslate nohighlight">\(T\)</span> ?</p></li>
</ul>
<p>We define a regularised filter (in the Fourier domain) by <em>bandlimitation</em>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\widehat{h}_{\alpha}(\xi) = \begin{cases} \widehat{h}(\xi) &amp; \text{for} &amp; |\xi|\leq \alpha^{-1} \\ 0 &amp; \text{for} &amp;|\xi| &gt; \alpha^{-1}\end{cases}.\end{split}\]</div>
<ul class="simple">
<li><p>Test its effect numerically on noisy data for <span class="math notranslate nohighlight">\(T = 2\)</span> using the code below. In particular, design an a-priori parameter selection rule <span class="math notranslate nohighlight">\(\alpha(\delta)\)</span> that ensures that the total error converges <em>and</em> stays below 1 (in relative sense). Does this rule do better than the naive choice <span class="math notranslate nohighlight">\(\alpha(\delta) = \delta\)</span>?</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># grid and parameters</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">delta</span> <span class="o">=</span> <span class="mf">1e-1</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">xi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">rfftfreq</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># define ground truth</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">heaviside</span><span class="p">(</span><span class="mi">2</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># define operator</span>
<span class="n">gh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">T</span><span class="o">*</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">xi</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">irfft</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">rfft</span><span class="p">(</span><span class="n">u</span><span class="p">)</span><span class="o">*</span><span class="n">gh</span><span class="p">)</span>

<span class="c1"># define regularised inverse</span>
<span class="n">w</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">alpha</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">piecewise</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="o">/</span><span class="n">alpha</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="o">/</span><span class="n">alpha</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">R</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">alpha</span><span class="p">,</span><span class="n">f</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">irfft</span><span class="p">(</span><span class="n">w</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">rfft</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="o">/</span><span class="n">gh</span><span class="p">)</span>

<span class="c1"># generate noisy data</span>
<span class="n">f_delta</span> <span class="o">=</span> <span class="n">K</span><span class="p">(</span><span class="n">u</span><span class="p">)</span> <span class="o">+</span> <span class="n">delta</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># reconstruction</span>
<span class="n">u_alpha_delta</span> <span class="o">=</span> <span class="n">R</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="n">f_delta</span><span class="p">)</span>

<span class="c1"># plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">u</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;ground truth&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">f_delta</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;noisy data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">u_alpha_delta</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;reconstruction&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ip_function_spaces_3_0.png" src="_images/ip_function_spaces_3_0.png" />
</div>
</div>
<div class="hint dropdown admonition">
<p class="admonition-title">Answer</p>
<ul class="simple">
<li><p>We can easily verify that it indeed satisfies the differential equation by computing <span class="math notranslate nohighlight">\(v_t\)</span> and <span class="math notranslate nohighlight">\(v_{xx}\)</span>. To show that it satisfies the initial condition, we take the limit <span class="math notranslate nohighlight">\(t\rightarrow 0\)</span> and use that <span class="math notranslate nohighlight">\(\lim_{t\rightarrow 0} \int g_t(x)u(x) \mathrm{d}x = u(0)\)</span>.</p></li>
<li><p>Setting <span class="math notranslate nohighlight">\(t = T\)</span> in the previous expression gives the desired result.</p></li>
<li><p>This follows from the <a class="reference external" href="https://en.wikipedia.org/wiki/Convolution_theorem">convolution theorem</a>.</p></li>
<li><p>The inverse of <span class="math notranslate nohighlight">\(K\)</span> is naively given by multiplication with <span class="math notranslate nohighlight">\((Fg_T)^{-1}\)</span> in the Fourier domain. We have <span class="math notranslate nohighlight">\(\widehat{g}_T(\xi) \propto e^{-T \xi^2}\)</span> so we can only usefully define the inverse on functions whose Fourier spectrum decays fast enough such that the inverse Fourier transform of <span class="math notranslate nohighlight">\(\widehat{f}/\widehat{g}_T\)</span> can be defined. Thus a solution does not exist for a large class of <span class="math notranslate nohighlight">\(f\)</span> and noise is amplified exponentially. This only gets worse for larger <span class="math notranslate nohighlight">\(T\)</span>.</p></li>
<li><p>See the code below. We compute the total error using the function <code class="docutils literal notranslate"><span class="pre">reconstruct</span></code> which computes noisy data and reconstructs using the regularised inverse. To get a more stable result we average over a number of random instances of the noise. Using <span class="math notranslate nohighlight">\(\alpha(\delta) = \delta\)</span> gives a (numerically) convergent result, however for large <span class="math notranslate nohighlight">\(\delta\)</span> it gives a very large error. Picking <span class="math notranslate nohighlight">\(\alpha(\delta)\)</span> to converge a bit slower allows one to keep the relative total error below 1. Note that we only show convergence of the error for one particular <span class="math notranslate nohighlight">\(u\)</span>, so we cannot conclude that this will work in general as in a previous exercise. If you like Fourier analysis and sampling theory, this may be a nice exercise.</p></li>
</ul>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># grid and parameters</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">delta</span> <span class="o">=</span> <span class="mf">1e-1</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">xi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">rfftfreq</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># define ground truth</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">heaviside</span><span class="p">(</span><span class="mi">2</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># define operator</span>
<span class="n">gh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">T</span><span class="o">*</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">xi</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">u</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">irfft</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">rfft</span><span class="p">(</span><span class="n">u</span><span class="p">)</span><span class="o">*</span><span class="n">gh</span><span class="p">)</span>

<span class="c1"># define regularised inverse</span>
<span class="n">w</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">alpha</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">piecewise</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span> <span class="p">[</span><span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">R</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">alpha</span><span class="p">,</span><span class="n">f</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">irfft</span><span class="p">(</span><span class="n">w</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">rfft</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="o">/</span><span class="n">gh</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">reconstruct</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">nsamples</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>

    <span class="n">error</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nsamples</span><span class="p">):</span>
        <span class="c1"># generate noisy data</span>
        <span class="n">f_delta</span> <span class="o">=</span> <span class="n">K</span><span class="p">(</span><span class="n">u</span><span class="p">)</span> <span class="o">+</span> <span class="n">delta</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

        <span class="c1"># reconstructions</span>
        <span class="n">u_alpha_delta</span> <span class="o">=</span> <span class="n">R</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="n">f_delta</span><span class="p">)</span>

        <span class="c1"># compute error</span>
        <span class="n">error</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">u</span> <span class="o">-</span> <span class="n">u_alpha_delta</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">u</span><span class="p">)</span><span class="o">/</span><span class="n">nsamples</span>

    <span class="c1">#</span>
    <span class="k">return</span> <span class="n">error</span>

<span class="n">alpha1</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">delta</span> <span class="p">:</span> <span class="n">delta</span>
<span class="n">alpha2</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">delta</span> <span class="p">:</span> <span class="mi">20</span><span class="o">*</span><span class="n">delta</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">8</span><span class="p">)</span>
<span class="n">ns</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">delta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">16</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">ns</span><span class="p">)</span>
<span class="n">error1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ns</span><span class="p">)</span>
<span class="n">error2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ns</span><span class="p">)</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ns</span><span class="p">):</span>
    <span class="n">error1</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">reconstruct</span><span class="p">(</span><span class="n">K</span><span class="p">(</span><span class="n">u</span><span class="p">),</span> <span class="n">delta</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">alpha1</span><span class="p">(</span><span class="n">delta</span><span class="p">[</span><span class="n">k</span><span class="p">]),</span><span class="n">nsamples</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">error2</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">reconstruct</span><span class="p">(</span><span class="n">K</span><span class="p">(</span><span class="n">u</span><span class="p">),</span> <span class="n">delta</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">alpha2</span><span class="p">(</span><span class="n">delta</span><span class="p">[</span><span class="n">k</span><span class="p">]),</span><span class="n">nsamples</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span><span class="mi">0</span><span class="o">*</span><span class="n">delta</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span><span class="s1">&#39;k--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span><span class="n">error1</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\alpha(\delta) = \delta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span><span class="n">error2</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\alpha(\delta) = 20\delta^{1/8}$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\delta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;relative error&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x7fcfea6b8760&gt;
</pre></div>
</div>
<img alt="_images/ip_function_spaces_5_1.png" src="_images/ip_function_spaces_5_1.png" />
</div>
</div>
</div>
</div>
<div class="section" id="assignments">
<h2><span class="section-number">3.6. </span>Assignments<a class="headerlink" href="#assignments" title="Permalink to this headline">¶</a></h2>
<div class="section" id="discretisation">
<h3><span class="section-number">3.6.1. </span>Discretisation<a class="headerlink" href="#discretisation" title="Permalink to this headline">¶</a></h3>
<p>In this exercise, we explore what happens when discretising the operator <span class="math notranslate nohighlight">\(K\)</span>. We’ll see that discretisation implicitly regularises the problem and that refining the discretisation brings out the inherent ill-posedness. Discretise <span class="math notranslate nohighlight">\(x_k = k\cdot h\)</span> with <span class="math notranslate nohighlight">\(k = 1, \ldots, n\)</span> and <span class="math notranslate nohighlight">\(h = 1/(n+1)\)</span>.</p>
<div class="math notranslate nohighlight">
\[
Ku(x_i)=\int_0^{x_i} u(y)\mathrm{d}y \approx h\sum_{j=0}^n k_{ij} u(x_j),
\]</div>
<p>with <span class="math notranslate nohighlight">\(k_{ij} = k(x_i,x_j) = H(x_i - x_j)\)</span>, giving an <span class="math notranslate nohighlight">\(n\times n\)</span> lower triangular matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
K = h\cdot\left(\begin{matrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 &amp; \ldots \\ \vdots &amp; &amp; &amp;\ddots \end{matrix}\right)
\end{split}\]</div>
<ol class="simple">
<li><p>Compute the SVD for various <span class="math notranslate nohighlight">\(n\)</span> and compare the singular values and vectors to the ones of the continuous operator. What do you notice?</p></li>
<li><p>Take <span class="math notranslate nohighlight">\(f(x) = x^3 + \epsilon\)</span> with <span class="math notranslate nohighlight">\(\epsilon\)</span> is normally distributed with mean zero and variance <span class="math notranslate nohighlight">\(\delta^2\)</span>. Investigate the accuracy of the reconstruction (use <code class="docutils literal notranslate"><span class="pre">np.linalg.solve</span></code> to solve <span class="math notranslate nohighlight">\(Ku = f\)</span>). Note that the exact solution is given by <span class="math notranslate nohighlight">\(u(x) = 3x^2\)</span>. Do you see the regularizing effect of <span class="math notranslate nohighlight">\(n\)</span>?</p></li>
</ol>
<p>The code to generate the matrix and its use are given below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">getK</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">h</span><span class="p">,</span><span class="mi">1</span><span class="o">-</span><span class="n">h</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">h</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">)))</span>

    <span class="k">return</span> <span class="n">K</span><span class="p">,</span><span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">delta</span> <span class="o">=</span> <span class="mf">1e-3</span>

<span class="n">K</span><span class="p">,</span><span class="n">x</span> <span class="o">=</span> <span class="n">getK</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">u</span> <span class="o">=</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span> <span class="o">+</span> <span class="n">delta</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">ur</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="n">f</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;|u - ur| = &#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">u</span> <span class="o">-</span> <span class="n">ur</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">u</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;true solution&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">ur</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;reconstruction&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>|u - ur| =  3.8617205416499942
</pre></div>
</div>
<img alt="_images/ip_function_spaces_9_1.png" src="_images/ip_function_spaces_9_1.png" />
</div>
</div>
</div>
<div class="section" id="convolution-on-a-finite-interval">
<h3><span class="section-number">3.6.2. </span>Convolution on a finite interval<a class="headerlink" href="#convolution-on-a-finite-interval" title="Permalink to this headline">¶</a></h3>
<p>We can define convolution with a Gaussian kernel on a finite interval <span class="math notranslate nohighlight">\([0,\pi]\)</span> through the initial boundary-value problem</p>
<div class="math notranslate nohighlight">
\[
v_t = v_{xx}, \quad v(t,0) = v(t,\pi) = 0,\quad v(0,x) = u(x)
\]</div>
<p>with <span class="math notranslate nohighlight">\(f(x) = v(1,x)\)</span>. The solution of the initial boundary-value problem is given by</p>
<div class="math notranslate nohighlight">
\[
v(t,x) = \sum_{k=1}^{\infty} a_k\exp(- k^2 t)\sin(k x),
\]</div>
<p>with <span class="math notranslate nohighlight">\(a_k\)</span> are the Fourier sine coefficients of <span class="math notranslate nohighlight">\(u\)</span>:</p>
<div class="math notranslate nohighlight">
\[
a_k = \langle u, \sin(k\cdot) \rangle = \frac{2}{\pi}\int_0^{\pi} u(x) \sin (k x) \mathrm{d}x.
\]</div>
<p>Assume that we use <span class="math notranslate nohighlight">\( \langle u, v \rangle = \frac{2}{\pi}\int_0^{\pi} u(x) v(x) \mathrm{d}x\)</span> as inner product and define the forward operator <span class="math notranslate nohighlight">\(f = Ku\)</span> in terms of the solution of the IBVP as <span class="math notranslate nohighlight">\(f(x) = v(1,x)\)</span>.</p>
<hr class="docutils" />
<p><strong>1.</strong> Give the singular system of <span class="math notranslate nohighlight">\(K\)</span>, i.e., find <span class="math notranslate nohighlight">\((\sigma_k, u_k, v_k)\)</span> such that <span class="math notranslate nohighlight">\(Ku(x)\)</span> can be expressed as</p>
<div class="math notranslate nohighlight">
\[
Ku(x) = \sum_{k=0}^\infty \sigma_k \langle u, v_k \rangle u_k(x).
\]</div>
<hr class="docutils" />
<p>We can now define a <em>regularised</em> pseudo-inverse through the variational problem</p>
<div class="math notranslate nohighlight">
\[
\min_{u} \|Ku - f\|^2 + \alpha R(u),
\]</div>
<p>where we investigate two types of regularisation</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(R(u) = \|u\|^2,\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(R(u) = \|u'\|^2.\)</span></p></li>
</ol>
<hr class="docutils" />
<p><strong>2.</strong> Show that these lead to the following regularised pseudo-inverses</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(K_{\alpha}^\dagger f = \sum_{k=0}^\infty \frac{1}{\sigma_k + \alpha\sigma_k^{-1}}\langle f, u_k \rangle v_k(x).\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(K_{\alpha}^\dagger f = \sum_{k=0}^\infty \frac{1}{\sigma_k + \alpha k^2\sigma_k^{-1}}\langle f, u_k \rangle v_k(x)\)</span></p></li>
</ol>
<p><strong>hint:</strong> you can use the fact that the <span class="math notranslate nohighlight">\(v_k\)</span> form an orthonormal basis for functions on <span class="math notranslate nohighlight">\([0,\pi]\)</span> and hence express the solution in terms of this basis.</p>
<hr class="docutils" />
<p>We can now study the need for regularisation, assuming that the Fourier coefficients <span class="math notranslate nohighlight">\(f_k = \langle f, u_k \rangle\)</span> of <span class="math notranslate nohighlight">\(f\)</span> are given.</p>
<p><strong>3.</strong> Determine which type of regularisation (if any) is needed to satisfy the Picard condition in the following cases (you can set <span class="math notranslate nohighlight">\(\alpha = 1\)</span> for this analysis)</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(f_k = \exp(-2 k^2)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f_k = k^{-1}\)</span></p></li>
</ol>
<hr class="docutils" />
<p><strong>4.</strong> Compute the bias and variance for <span class="math notranslate nohighlight">\(u(x) = \sin(k x)\)</span> and  measurements <span class="math notranslate nohighlight">\(f^{\delta}(x) = Ku(x) + \delta \sin(\ell x)\)</span> for fixed <span class="math notranslate nohighlight">\(k &lt; \ell\)</span> and <span class="math notranslate nohighlight">\(\delta\)</span>. Plot the bias and variance for well-chosen <span class="math notranslate nohighlight">\(k,\ell\)</span> and <span class="math notranslate nohighlight">\(\delta\)</span> and discuss the difference between the two types of regularization.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="discrete_ip_regularization.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">2. </span>Discrete Inverse Problems and Regularisation</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="statistical_perspective.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4. </span>A statistical perspective on inverse problems</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Tristan van Leeuwen and Christoph Brune (CC BY-NC 4.0)<br/>
    
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>